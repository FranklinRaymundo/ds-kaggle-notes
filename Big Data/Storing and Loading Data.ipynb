{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage\n",
    "\n",
    "These are ways I found to save store data so that they are as LIGHT as possible\n",
    "\n",
    "### A. CSV (with or without GZIP)\n",
    "\n",
    "- This is super slow but works out of the box. \n",
    "- I don't find any reason to use it except for fast visualization when the data is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  one  two\n",
       "0      2  1.0  1.0\n",
       "1      6  2.0  2.0\n",
       "2      7  3.0  3.0\n",
       "3      9  4.0  4.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('database.csv', compression='gzip', index=True, index_label='index')\n",
    "pd.read_csv('database.csv', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. NPZ\n",
    "\n",
    "This one saves several arrays or multidimensional arrays into a single file in compressed.\n",
    "- It's the best solution I know so far.\n",
    "- **It works for both dataframes and series.**\n",
    "- It ignores indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 2.],\n",
       "       [3., 3.],\n",
       "       [4., 4.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez_compressed('df.npz', df=df)\n",
    "np.load('df.npz')['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez_compressed('ts.npz', ts=ts)\n",
    "np.load('ts.npz')['ts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. PARQUET\n",
    "\n",
    "It has lots of dependecies, but after having them, it outperforms npz.\n",
    "- **It only works with dataframes**.\n",
    "- It produces the lightest compressions, in general.\n",
    "- It doesn't ignore indexes.\n",
    "- It's the fastest way.\n",
    "- **It keeps the column names**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  two\n",
       "2  1.0  1.0\n",
       "6  2.0  2.0\n",
       "7  3.0  3.0\n",
       "9  4.0  4.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_parquet('database.parquet', compression='gzip')\n",
    "pd.read_parquet('database.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Usually competitors **store their data after preprocessing and feature engineering**, but it might be too big, and also there might be a lot of kernel restarts, so they do the next:\n",
    "\n",
    "hdf5 and npy files are loaded faster.\n",
    "\n",
    "### A. Data is stored in 64 bit arrays. Downcast it to 32 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before 144\n",
      "Memory usage after 112\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "iris = load_iris()\n",
    "data = {'col1' : [1., 2., 3., 4.], 'col2' : [4., 3., 2., 1.]}\n",
    "df = pd.DataFrame(data)\n",
    "print('Memory usage before {}'.format(df.memory_usage(index=True).sum()))\n",
    "downcast_dtypes(df)\n",
    "print('Memory usage after {}'.format(df.memory_usage(index=True).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Converting csv/txt to hdf5 (for panda dataframes)/npy (for non-bit arrays) for faster loading\n",
    "\n",
    "- **HDF5(Hierarchical Data Format):** Only when working with large scale datasets which don't fit in memory. This file type will be heavier than others.\n",
    "- **NPY:** Numpy binary file. I'm not sure if you can read it by chunks as with HDF (TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   one  two\n",
      "2  1.0  1.0\n",
      "6  2.0  2.0\n",
      "7  3.0  3.0\n",
      "9  4.0  4.0\n",
      "   one  two\n",
      "0  3.0  3.0\n",
      "1  4.0  4.0\n"
     ]
    }
   ],
   "source": [
    "data = {'one' : [1., 2., 3., 4.],\n",
    "        'two' : [1., 2., 3., 4.]}\n",
    "df = pd.DataFrame(data, index=[2,6,7,9])\n",
    "\n",
    "data = {'one' : [3., 4.],\n",
    "        'two' : [3., 4.]}\n",
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "# WRITE\n",
    "df.to_hdf('database.h5', key='df', mode='a')\n",
    "df2.to_hdf('database.h5', key='df2', mode='a')\n",
    "\n",
    "# READ\n",
    "df_ = pd.read_hdf('database.h5', 'df')\n",
    "df2_ = pd.read_hdf('database.h5', 'df2')\n",
    "print(df_)\n",
    "print(df2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1., 2., 3., 4.]\n",
    "ts = pd.Series(data)\n",
    "\n",
    "# WRITE\n",
    "np.save(arr=ts, file='ts.npy')\n",
    "# READ\n",
    "ts_ = np.load('ts.npy')\n",
    "ts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Large datasets can be processed in chunks (TODO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
