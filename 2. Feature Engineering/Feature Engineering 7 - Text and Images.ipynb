{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 cases when dealing with text or images.\n",
    "\n",
    "## When there are only text or images as features\n",
    "\n",
    "#### Text\n",
    "- Text Feature extraction\n",
    "- Kaggle's Allen Institute Challenge\n",
    "\n",
    "#### Images\n",
    "- Convolutional NN\n",
    "- Kaggle's Data Science Bowl\n",
    "\n",
    "## When text or images are additional data\n",
    "\n",
    "We can extract features complementary to the other features.\n",
    "\n",
    "- Kaggle's Avito Duplicate Ads Detection\n",
    "- Kaggle's TRADESHIFT\n",
    "- Kaggle's Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "These topics text and image processing should be covered separately, so this is more like a quick review.\n",
    "\n",
    "### 1. Lowercase\n",
    "Convert strings to lowercase.\n",
    "\n",
    "### 2. Stemming\n",
    "A stemmer should identify the string \"cats\" (and possibly \"catlike\", \"catty\" etc.) as based on the root \"cat\". It would seem like it only shortens words.\n",
    "- It operates on a single word without knowledge of the context.\n",
    "- democracy, democratic, and democratization ---> democr\n",
    "- saw ---> s\n",
    "    \n",
    "### 3. Lemmatization\n",
    "For example, the word \"better\" has \"good\" as its lemma. \n",
    "- It converts words of a sentence to their dictionary form. For example, given the words amusement, amusing, and amused, the lemma for each and all would be amuse.\n",
    "- democracy, democratic, and democratization ---> democracy\n",
    "- saw ---> see or saw (depending on the context\n",
    "\n",
    "### 4. Stopwords\n",
    "Examples:\n",
    "- Articles or prepositions\n",
    "- Very common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction\n",
    "\n",
    "![](images/textfeatures.png)\n",
    "\n",
    "There are 2 main ways\n",
    "\n",
    "#### 1. BoW\n",
    "\n",
    "- https://github.com/rgap/simbig2016-facebook-reactions/blob/master/lectures/TF-IDF.ipynb\n",
    "- **TF-iDF** is the most known **Postprocessing Method** after BoW. There are different variants of TF-iDF that may work better depending on the data.\n",
    "- **N-grams** may be useful. Add columns corresponding to words and also columns corresponding to 'n' consequent words.\n",
    "- N-grams can also refer to sequences of caracters.\n",
    "\n",
    "##### Features\n",
    "- It produces very large vectors\n",
    "- The meaning of each value in the vector is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2vec\n",
    "- https://github.com/rgap/simbig2016-facebook-reactions/blob/master/lectures/Doc2Vec.ipynb\n",
    "- Implementations of this approach:\n",
    "    - For words: Word2vec, Glove, FastText, etc\n",
    "    - For sentences:\n",
    "        - Use the mean, or the sum of word vectors.\n",
    "        - Use Doc2vec\n",
    "- Training can be slow, so there are **pre-trained models**\n",
    "\n",
    "##### Features\n",
    "- It produces relatively small vectors\n",
    "- **Values in the vector can be interpreted only in some cases**\n",
    "- Words with similar meaning often have similar embeddings (vector representations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Feature Extraction\n",
    "\n",
    "**Convolutional Neural Networks (CNN)** can give a vector representation for an image.\n",
    "\n",
    "**Descriptors:** The outputs of any layer are called descriptors.\n",
    "\n",
    "When getting the output of the network, besides the output of the last layer, it also gives the output for the inner layers.\n",
    "\n",
    "- Descriptors from later layers are about what the network solves.\n",
    "- Descriptors from earlier layers have more task independent information and can be used to perform other tasks.\n",
    "\n",
    "![](networkCNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features comes essentially within the training step of a CNN.\n",
    "\n",
    "#### a. Train a network from scratch\n",
    "\n",
    "- It is usually better because it allows to tune more parameters.\n",
    "\n",
    "#### b. Use a pre-trained network\n",
    "- **Fine-tuning** = process of pre-trained model tuning.\n",
    "- It's useful if we have **too little data**.\n",
    "- Mostly used when the only data given is images.\n",
    "- When the task we are solving is similar to the task the model was trained on.\n",
    "- When solving medical-specific tasks you may use these pre-trained networks.\n",
    "    - VGG\n",
    "    - RestNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation\n",
    "\n",
    "This is about adding training images to train a better network.\n",
    "\n",
    "- Adding images rotated by 90 degrees.\n",
    "- Adding images rotated by 180 degrees.\n",
    "- Adding denoised images.\n",
    "- etc\n",
    "\n",
    "Augmented images can be used in both train and test time\n",
    "- At **train time** to increase the amount of training data\n",
    "- At **test time** to average predictions for one augmented sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
